# 基本概念

[精确度、召回率、F1分数](https://blog.csdn.net/AlexFaker/article/details/108286805)
[傅里叶分析](https://blog.csdn.net/qq_36810398/article/details/102621639)
[基线模型](https://blog.csdn.net/weixin_46163097/article/details/123933649)

[R2和MAPE指标](https://blog.csdn.net/weixin_40651515/article/details/105930868)

# Can LLMs Understand Time Series Anomalies-LLMs在时间序列异常检测

# Temporal Data Meets LLM-Explainable Financial Time Series Forecasting-LLMs金融时间序列预测

### [零样本和少样本](https://blog.csdn.net/weixin_42010722/article/details/131182669)


### [链式思考](https://blog.csdn.net/xw555666/article/details/136450679)


### [股票回报](https://baike.baidu.com/item/%E8%82%A1%E7%A5%A8%E5%9B%9E%E6%8A%A5%E7%8E%87/12742678)


### [梯度提升树模型](https://blog.csdn.net/qq_44665283/article/details/137559749)


### [基线模型](https://blog.csdn.net/weixin_46163097/article/details/123933649)


# Time Series Forecasting with LLMs-LLMs在时间序列预测强趋势和周期性


# TIME-LLM TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS-TIME-LLM框架的核心思想是将输入时间序列重新编程为文本原型表示

### RevIN：

RevIN是指一种可逆实例归一化方法，用于时间序列预测任务中的数据预处理。这种方法能够解决由于时间序列数据的分布偏移问题导致的预测不准的问题。

* **具体含义**：
  * **定义**：RevIN是一种特殊的归一化技术，它在时间序列预测模型中对输入数据进行归一化处理，并且可以在推理阶段将数据反归一化回去。
  * **机制**：在训练阶段，RevIN会计算每个样本过去窗口的统计量（如均值和方差），并根据这些统计量来标准化输入数据。同时，它还会记录下这些统计量以便在测试阶段使用。
  * **优势**：通过这种方式，RevIN能够更好地捕捉到时间序列数据中随时间变化的统计特性，从而提高模型的预测准确性。
* **应用范围**：
  * **时间序列预测**：RevIN特别适用于那些需要对具有分布偏移的时间序列数据进行准确预测的任务。
  * **深度学习模型**：RevIN可以被集成到各种基于深度学习的时间序列预测模型中，以改善其性能。
* **与传统归一化方法的区别**：
  * **静态归一化**：传统的归一化方法通常基于整个数据集的统计信息来进行归一化，这可能导致在推理阶段出现数据分布的概念性漂移。
  * **动态归一化**：而RevIN则是在每个样本的基础上进行归一化的，这样可以更精确地反映时间序列数据的局部特征，避免了概念性漂移的问题。
* **进一步拓展**：
  * **与其他技术结合**：除了直接应用于时间序列预测之外，RevIN还可以与其他先进的技术结合使用，例如通过结合注意力机制或自编码器等方法来增强模型的能力。
  * **研究进展**：尽管RevIN已经在一些领域展示了其有效性，但研究人员仍在探索如何进一步改进该方法，使其能够在更多复杂的情况下发挥作用。

### [通道独立的策略](https://cloud.tencent.com/developer/article/2451835)：

* **通道独立建模方法**：将多元时间序列分解为多个单一时间序列，并应用统一的单变量预测模型进行处理。这种方法对非平稳数据有强大的鲁棒性，但未能考虑通道间的相互关联，限制了性能的进一步优化。例如，在天气预测中，若仅用通道独立策略预测温度，虽能一定程度上保证模型对温度数据本身的鲁棒性，但忽略了湿度、风速和气压等与温度相关因素的信息，可能影响预测精度。
* **结合通道混合的改进策略**：一些模型在采用通道独立策略的基础上，通过特定机制引入通道间的交互，以提升性能。如PDMLP模型，采用通道独立策略处理多变量时间序列，同时通过通道混合促进变量之间的语义信息交换，取得了优异的性能。还有TimeCHEAT模型，其在每个patch内部采用channel dependent建模策略，通过二部图的学习建模内部各个变量和各个时间戳的关系；而对于不同patch之间，则使用channel independent策略，用一个全局Transformer专注于建模时间维度间的关系，避免学习长周期channel之间依赖关系带来的过拟合等问题。

### cross-attention：

cross-attention是指一种在Transformer架构中的注意力机制，它允许模型在一个模态的特征上关注另一个模态的特征。具体来说，在原文中，cross-attention被用来将时间序列输入特征与自然语言文本域上的词嵌入对齐。

* cross-attention的具体含义：
  * 允许不同模态之间进行信息交互
  * 一个序列作为Query输入，定义了输出长度；另一个序列产生Key和Value输入，用于计算注意力权重
  * 输出序列的维度和长度与Query序列相同
* 延伸拓展：
  * 在多模态任务中非常有用，例如图像字幕、多模式机器翻译等
  * 与自注意力机制（self-attention）的主要区别在于QKV的来源不同，后者来自同一个序列
  * 通过引入cross-attention机制，可以更好地处理不同模态的数据，实现更复杂的任务，如图像和文本之间的匹配
  * 在一些场景下，可以利用cross-attention机制进行特征融合，以获得更丰富、更全面的表征
  * 可以应用于各种深度学习任务，如图像编辑技术中的Prompt-to-Prompt方法，通过修改cross-attention层中的像素到文本交互来控制生成图像的空间布局和几何形状

### text prototypes（语义原型）：

text prototypes是指用于表示时序数据变化特性的简化文本表示，通过线性组合词汇表中的词来生成。具体来说：

* text prototypes是通过对原始词汇表进行线性组合得到的简化表示。
  * 这些原型的数量远小于原始词汇表的大小。
  * 每个原型可以代表特定类型的时序数据特征，例如“短暂上升”或“缓慢下降”。

text prototypes在模型中的作用包括：

* 简化输入数据与语言模型之间的对齐过程。
  * 使用这些原型代替原始词汇表，使得时序数据更容易被语言模型理解。
* 提高模型处理时序数据的能力。
  * 通过选择最相关的原型，模型能够更准确地捕捉到时序数据的特性。

此外，text prototypes的应用范围不仅限于时间序列预测任务，还可以应用于其他需要将非语言数据转换为语言表示的任务中。
